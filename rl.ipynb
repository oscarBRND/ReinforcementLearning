{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a81a2c1",
   "metadata": {},
   "source": [
    "# RL I: Environnement \n",
    "\n",
    "## 1- Le dictionnaire P\n",
    "\n",
    "P:{$S_0$: {$a_0$: [(s'), (s'), ...], $a_1$: [(s'), (s'), ...]},\n",
    "   $S_1$: {$a_0$: [(s'), (s'), ...], $a_1$: [(s'), (s'), ...]}}\n",
    "\n",
    "P[S][a] --> [(s'), (s'), ...]\n",
    "\n",
    "(s') = (p(s'|s,a), s', r(s,a,s'), done?)\n",
    "\n",
    "## 2- Exemple du Row Gridword\n",
    "\n",
    "|-100|0|0|0|+10|\n",
    "|||s|||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "500600e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridworldRow():\n",
    "    def __init__(self, size=5):\n",
    "        self.size = size\n",
    "\n",
    "        self.nS = size  # number of states\n",
    "        self.nA = 2     # number of actions: left, right\n",
    "\n",
    "        self.MAX_X = size - 1 \n",
    "\n",
    "        P = {}\n",
    "\n",
    "        for s in range(self.nS):\n",
    "            dynamics_s = {}\n",
    "            for a in range(self.nA):\n",
    "                s_prime_list = []\n",
    "\n",
    "                p = 1 if s!=0 and s != self.MAX_X else 0\n",
    "\n",
    "                if a == 0:  # left\n",
    "                    s_prime = max(s - 1, 0)\n",
    "                else:\n",
    "                    s_prime = min(s + 1, self.MAX_X)\n",
    "                \n",
    "                if (s_prime == 0):\n",
    "                    reward = -100\n",
    "                    done = True\n",
    "                elif (s_prime == self.MAX_X):\n",
    "                    reward = 10\n",
    "                    done = True\n",
    "                else:\n",
    "                    reward = 0\n",
    "                    done = False\n",
    "\n",
    "                s_prime_list.append((p, s_prime, reward, done))\n",
    "                dynamics_s.update({a: s_prime_list})\n",
    "            P.update({s: dynamics_s})\n",
    "        self.P = P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9446d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(0, 0, -100, True)], 1: [(0, 1, 0, False)]},\n",
       " 1: {0: [(1, 0, -100, True)], 1: [(1, 2, 0, False)]},\n",
       " 2: {0: [(1, 1, 0, False)], 1: [(1, 3, 0, False)]},\n",
       " 3: {0: [(1, 2, 0, False)], 1: [(1, 4, 10, True)]},\n",
       " 4: {0: [(0, 3, 0, False)], 1: [(0, 4, 10, True)]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = GridworldRow()\n",
    "env.P  # Access the transition dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96d0f9a",
   "metadata": {},
   "source": [
    "# RL II: Implémentation de Policy Evaluation, Improvement, Iteration\n",
    "\n",
    "## 1. Policy Evaluation\n",
    "\n",
    "Calcul de la value function $v_{\\pi}$ de la policy aléatoire $\\pi$ dans le Row GridWorld\n",
    "\n",
    "L'algorithme de *Policy Evaluation* calcule $v_{\\pi} pour $\\pi$, pour chaque state S. Pour ce faire il applique itérativement la première équation d'espérance de Bellman.\n",
    "\n",
    "$$ V(s) \\leftarrow \\sum_a \\pi(a|s)\\sum_{s'}p(s'|s, a)[r(s, a, s') + \\gamma V(s')], \\forall s$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e161227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "env = GridworldRow()\n",
    "pi = np.ones([env.nS, env.nA]) / env.nA     # uniform random policy\n",
    "V = np.zeros([env.nS, 1])\n",
    "\n",
    "gamma = 0.9 # discount factor\n",
    "theta = 1e-10 # stopping threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a3fb09",
   "metadata": {},
   "source": [
    "On décompose le problème :\n",
    "\n",
    "$$\n",
    "V_\\pi(s) \\;=\\; \\sum_{a}\\pi(a\\mid s)\\,Q_\\pi(s,a), \\quad \\forall s.\n",
    "$$\n",
    "\n",
    "avec\n",
    "$$\n",
    "Q_\\pi(s,a)\\;=\\;\\sum_{s'} p(s'\\mid s,a)\\Bigl[r(s,a,s')+\\gamma\\,V_\\pi(s')\\Bigr].\n",
    "$$\n",
    "\n",
    "Calculons donc $Q_\\pi(s,a)$ à partir de $V_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872a75d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_q_value_s_a(env, V, s, a, gamma):\n",
    "    q_sa = 0\n",
    "    for p, s_prime, r, done in env.P[s][a]:\n",
    "        q_sa += p*(r + gamma*V[s_prime])\n",
    "    return float(q_sa[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d35cf71",
   "metadata": {},
   "source": [
    "boucle d'update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "898b3685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.        ]\n",
      " [-71.62197274]\n",
      " [-43.68075301]\n",
      " [-16.62197274]\n",
      " [  0.        ]] 32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def policy_evaluation(env, pi, V, gamma=0.99, theta=1e-8, max_iter=10_000):\n",
    "    \"\"\"\n",
    "    Évalue une politique pi en calculant V_pi via des mises à jour itératives (Bellman expectation backup).\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    env : environnement (discret)\n",
    "        Doit exposer env.nS (nb d'états), env.nA (nb d'actions), et la dynamique utilisée\n",
    "        par compute_q_value_s_a(...).\n",
    "    pi : array-like, shape (nS, nA)\n",
    "        Politique stochastique : pi[s][a] = P(a|s).\n",
    "    V : np.ndarray, shape (nS,)\n",
    "        Valeur d'état initiale. Modifiée en place et aussi renvoyée.\n",
    "    gamma : float\n",
    "        Facteur d'actualisation.\n",
    "    theta : float\n",
    "        Seuil de convergence (norme infinie sur la variation de V).\n",
    "    max_iter : int\n",
    "        Garde-fou pour éviter une boucle infinie si problème de convergence.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    V : np.ndarray\n",
    "        Estimation de V_pi.\n",
    "    n_iter : int\n",
    "        Nombre d'itérations effectuées.\n",
    "    \"\"\"\n",
    "    V = np.asarray(V, dtype=float)  # assure le type float\n",
    "    pi = np.asarray(pi, dtype=float)\n",
    "\n",
    "    for n_iter in range(1, max_iter + 1):\n",
    "        delta = 0.0\n",
    "\n",
    "        for s in range(env.nS):\n",
    "            v_old = V[s]\n",
    "\n",
    "            v_new = 0.0\n",
    "            for a in range(env.nA):\n",
    "                prob_a = pi[s, a]\n",
    "                q_sa = compute_q_value_s_a(env, V, s, a, gamma)\n",
    "                v_new += prob_a * q_sa\n",
    "\n",
    "            delta = max(delta, abs(v_new - v_old)) \n",
    "            V[s] = v_new\n",
    "\n",
    "        # Critère de convergence\n",
    "        if delta < theta:\n",
    "            return V, n_iter\n",
    "\n",
    "    # Si on sort ici, on n'a pas convergé dans max_iter itérations\n",
    "    return V, n_iter\n",
    "\n",
    "\n",
    "v, n_it = policy_evaluation(env, pi, V)\n",
    "print(v, n_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f3cd9",
   "metadata": {},
   "source": [
    "## 2. Policy improvement\n",
    "\n",
    "Déduction de $\\pi$' grâce à $v_{\\pi}$ dans le Row Gridworld\n",
    "Il attribue à $\\pi$' pour chaque state, l'action qui maximize le return $q_{\\pi}(s, a)$ de la manière suivante:\n",
    "\n",
    "$$\\pi'(s) \\leftarrow argmax_a Q(s, a), \\forall s$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6261c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def policy_improvement(env, V, gamma=0.9):\n",
    "    \"\"\"\n",
    "    Améliore une politique à partir d'une fonction de valeur V (greedy policy improvement).\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    pi_new : np.ndarray, shape (nS, nA)\n",
    "        Politique déterministe greedy : pi_new[s, a*] = 1 pour l'action a* argmax_a Q(s,a).\n",
    "    \"\"\"\n",
    "    pi_new = np.zeros((env.nS, env.nA), dtype=float)\n",
    "\n",
    "    for s in range(env.nS):\n",
    "        # Calcule tous les Q(s,a) pour l'état s\n",
    "        q_values = np.zeros(env.nA, dtype=float)\n",
    "        for a in range(env.nA):\n",
    "            q_values[a] = compute_q_value_s_a(env, V, s, a, gamma)\n",
    "\n",
    "        best_a = int(np.argmax(q_values))\n",
    "        # Politique déterministe: 1 sur best_a, 0 ailleurs\n",
    "        pi_new[s, best_a] = 1.0\n",
    "\n",
    "    return pi_new\n",
    "\n",
    "\n",
    "new_pi = policy_improvement(env, V, gamma=0.9)\n",
    "new_pi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e4460d",
   "metadata": {},
   "source": [
    "## 3. Policy iteration\n",
    "\n",
    "Résolution complète du Row GridWorld\n",
    "\n",
    "Enchaînement de la tâche de *prédiction* et de *contrôle*. Prédiction: **Policy Evaluation**, Contrôle: **Policy improvement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea22ffb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. ]\n",
      " [ 8.1]\n",
      " [ 9. ]\n",
      " [10. ]\n",
      " [ 0. ]] [[1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]] 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def policy_iteration(env, V=None, pi=None, gamma=0.9, theta=1e-8, max_iter=10_000):\n",
    "    \"\"\"\n",
    "    Policy Iteration (itération sur les politiques) :\n",
    "    1) Policy Evaluation : calcule V^{pi}\n",
    "    2) Policy Improvement : pi <- greedy(V^{pi})\n",
    "    Jusqu'à stabilité de la politique.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    env : env discret \n",
    "    V : np.ndarray (nS,) ou None\n",
    "        Valeur initiale. Si None -> vecteur de zéros.\n",
    "    pi : np.ndarray (nS,nA) ou None\n",
    "        Politique initiale (stochastique). Si None -> uniforme.\n",
    "    gamma : float\n",
    "        Discount.\n",
    "    theta : float\n",
    "        Seuil de stabilité de la politique (on compare old_pi vs new_pi).\n",
    "    max_iter : int\n",
    "        Nombre max d'itérations de policy iteration.\n",
    "\n",
    "    Retour\n",
    "    ------\n",
    "    V : np.ndarray (nS,)\n",
    "    pi : np.ndarray (nS,nA)\n",
    "    n_outer : int (nombre d'itérations externes)\n",
    "    \"\"\"\n",
    "    # Initialisations robustes\n",
    "    if V is None:\n",
    "        V = np.zeros(env.nS, dtype=float)\n",
    "    else:\n",
    "        V = np.asarray(V, dtype=float)\n",
    "\n",
    "    if pi is None:\n",
    "        pi = np.ones((env.nS, env.nA), dtype=float) / env.nA\n",
    "    else:\n",
    "        pi = np.asarray(pi, dtype=float)\n",
    "\n",
    "    for n_outer in range(1, max_iter + 1):\n",
    "        old_pi = pi.copy() \n",
    "\n",
    "        # 1) Évaluation de la politique courante\n",
    "        V, _ = policy_evaluation(env, old_pi, V, gamma=gamma, theta=theta)\n",
    "\n",
    "        # 2) Amélioration (politique greedy)\n",
    "        pi = policy_improvement(env, V, gamma=gamma)\n",
    "\n",
    "        # 3) Test de stabilité de la politique\n",
    "        delta = np.max(np.abs(pi - old_pi))\n",
    "\n",
    "        if delta < theta:\n",
    "            return V, pi, n_outer\n",
    "\n",
    "    return V, pi, n_outer\n",
    "\n",
    "\n",
    "V_star, pi_star, n = policy_iteration(env, V, pi)\n",
    "print(V_star, pi_star, n)\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
